prep_word2vec(origin='features.txt',destination = 'vectors.txt',lowercase = T)
model = train_word2vec("vectors.txt","feature_vectors.bin",vectors=300,threads=4,window=5,iter=5,negative_samples=0)
word2vec = read.vectors("feature_vectors.bin")
View(word2vec)
head(word2vec)
dim(word2vec)
word2vecDf=as.data.frame(word2vec)
word2vecDf
head(word2vecDf)
colnames(word2vecDf)
word2vecDf.index
word2vec[1]
word2vec[0]
View(word2vecDf)
?prep_word2vec
model = train_word2vec("features.txt","feature_vectors.bin",vectors=300,threads=4,window=5,iter=5,negative_samples=0)
word2vec = read.vectors("feature_vectors.bin")
word2vecDf=as.data.frame(word2vec)
View(word2vecDf)
a=unique(all)
a=as.data.frame(unique(all))
View(a)
word2vecDf[0]
word2vec[1:67]
word2vec[1:67]
word2vec[1]
word2vec[1,1]
model = train_word2vec("features.txt","feature_vectors.bin",vectors=300,threads=4,window=5,iter=5,negative_samples=0,sample=0.001,binary=1)
?word2vec
?train_word2vec
model = train_word2vec("features.txt","feature_vectors.bin",vectors=300,threads=4,window=5,iter=5,negative_samples=5)
length(all)
all=tolower(all)
all=gsub("[[:punct:]]", "", all)
all
write(all,'features.txt')
prep_word2vec(origin='features.txt',destination = 'vectors.txt',lowercase = T)
model = train_word2vec("features.txt","feature_vectors.bin",vectors=300,threads=4,window=5,iter=5,negative_samples=5)
write(all,'features.txt')
model = train_word2vec("features.txt","feature_vectors.bin",vectors=300,threads=4,window=5,iter=5,negative_samples=5)
model = train_word2vec("features.txt","feature_vectors.bin",vectors=300,threads=4,window=5,iter=5,negative_samples=5)
write(all,'features.txt')
model = train_word2vec("features.txt","feature_vectors.bin",vectors=300,threads=4,window=5,iter=5,negative_samples=0)
concat_features=so_data %>%
select(IDE,HaveWorkedLanguage,Country,HaveWorkedDatabase,HaveWorkedFramework,HaveWorkedPlatform,WantWorkLanguage,WantWorkFramework,WantWorkDatabase,WantWorkPlatform) %>%
filter(Country=='Pakistan') %>%
select(-Country) %>%
na.omit(.) %>%
mutate_if(is.factor,as.character) %>%
mutate(IDE=paste(IDE,";",sep="")) %>%
mutate(HaveWorkedLanguage=paste(HaveWorkedLanguage,";",sep="")) %>%
mutate(HaveWorkedDatabase=paste(HaveWorkedDatabase,";",sep="")) %>%
mutate(HaveWorkedFramework=paste(HaveWorkedFramework,";",sep="")) %>%
mutate(WantWorkLanguage=paste(WantWorkLanguage,";",sep="")) %>%
mutate(WantWorkDatabase=paste(WantWorkDatabase,";",sep="")) %>%
mutate(WantWorkFramework=paste(WantWorkFramework,";",sep="")) %>%
mutate(concat_have=paste(IDE,HaveWorkedLanguage,HaveWorkedDatabase,HaveWorkedFramework,HaveWorkedPlatform)) %>%
mutate(concat_have=gsub(" ","",concat_have)) %>%
mutate(concat_want=paste(IDE,WantWorkLanguage,WantWorkDatabase,WantWorkFramework,WantWorkPlatform)) %>%
mutate(concat_want=gsub(" ","",concat_want))
vectorAllWant=concat_features %>%
pull(concat_want)
vectorAllHave=concat_features %>%
pull(concat_have)
embeddingAllWant=unlist(strsplit(as.character(vectorAllWant),';'))
embeddingAllHave=unlist(strsplit(as.character(vectorAllHave),';'))
all=c(embeddingAllWant,embeddingAllHave)
doc=Corpus(VectorSource(all))
dtm <- TermDocumentMatrix(doc)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
write(all,'features.txt')
model = train_word2vec("features.txt","feature_vectors.bin",vectors=300,threads=4,window=5,iter=5,negative_samples=0)
?tcm
install.packages('text2vec')
?create_tcm
library(text2vec)
?create_tcm
v = create_vocabulary(all)
View(v)
vectorizer = vocab_vectorizer(v, grow_dtm = F, skip_grams_window = 5)
?vocab_vectorizer
vectorizer = vocab_vectorizer(v, grow_dtm = F, skip_grams_window = 5)
vectorizer = vocab_vectorizer(v)
vectorizer
tcm=create_tcm(all,vectorizer)
?create_tcm
it = itoken(all)
it
tcm=create_tcm(it,vectorizer)
?itoken
all
unlist(all)
as.data.frame(all)
tmp=as.data.frame(all)
View(tmp)
it = itoken(tmp$all)
View(tmp)
tmp = tmp %>% mutate(all=as.character(all))
it = itoken(tmp$all)
it
v = create_vocabulary(it)
View(v)
vectorizer = vocab_vectorizer(v)
tcm=create_tcm(it,vectorizer)
tcm
fit <- glove(tcm = tcm,
word_vectors_size = 50,
x_max = 10, learning_rate = 0.2,
num_iters = 15)
fit <- GloVe(tcm = tcm,
word_vectors_size = 50,
x_max = 10, learning_rate = 0.2,
num_iters = 15)
fit <- glove(tcm = tcm,
word_vectors_size = 50,
x_max = 10, learning_rate = 0.2,
num_iters = 15)
vectorizer = vocab_vectorizer(v, grow_dtm = F, skip_grams_window = 5)
vectorizer = vocab_vectorizer(v)
vectorizer
word_array
clear
tmp=as.data.frame(all)
tmp = tmp %>% mutate(all=as.character(all)) %>% tolower(.)
tmp
it = itoken(tmp$all)
tmp=as.data.frame(all)
tmp = tmp %>% mutate(all=as.character(all)) %>% mutate(all=tolower(all))
it = itoken(tmp$all)
v = create_vocabulary(it)
v
vectorizer = vocab_vectorizer(v)
tcm=create_tcm(it,vectorizer)
v = create_vocabulary(it)  prune_vocabulary(term_count_min = 5)
v = create_vocabulary(it) %>%  prune_vocabulary(term_count_min = 5)
vectorizer = vocab_vectorizer(v)
tcm=create_tcm(it,vectorizer)
vectorizer = vocab_vectorizer(v,skip_grams_window = 5)
?crate_tcm
?create_tcm
tcm=create_tcm(it,vectorizer,skip_grams_window=5)
tcm=create_tcm(it,vectorizer,skip_grams_window=5L)
?create_tcm
tcm
word2vec
word2vec[1:67]
word2vec[1:66]
?plot
??Rtsne
Rtsne(word2vec)
reduction <- Rtsne(as.matrix(word2vec), dims = 2, initial_dims = 50,
perplexity = 300, theta = 0.5, check_duplicates = F,
pca = F, max_iter = 1000, verbose = F,
is_distance = F, Y_init = NULL)
word2vec
word2vecDf=as.data.frame(word2vec)
reduction <- Rtsne(as.matrix(word2vecdf), dims = 2, initial_dims = 50,
perplexity = 300, theta = 0.5, check_duplicates = F,
pca = F, max_iter = 1000, verbose = F,
is_distance = F, Y_init = NULL)
reduction <- Rtsne(as.matrix(word2vec), dims = 2, initial_dims = 300,
perplexity = 300, theta = 0.5, check_duplicates = F,
pca = F, max_iter = 1000, verbose = F,
is_distance = F, Y_init = NULL)
reduction <- Rtsne(as.matrix(word2vec), dims = 2, initial_dims = 50,
perplexity = 21, theta = 0.5, check_duplicates = F,
pca = F, max_iter = 1000, verbose = F,
is_distance = F, Y_init = NULL)
reduction
df <- as.data.frame(reduction$Y)
View(df)
rows <- rownames(word2vec)
rows
rownames(df) <- rows
View(df)
View(df)
df % filter(rownames(df) == '</s>')
df %>% filter(rownames(df) == '</s>')
df %>% filter(rownames(df) != '</s>')
df=df %>% filter(rownames(df) != '</s>')
View(df)
rows
rows[2:]
rows[2:66]
rows[2:67]
rows[2:68]
rows[2:67]
rownames(df) <- rows[2:67]
View(df)
ggplot(df) +
geom_point(aes(x = V1, y = V2), color = "red") +
geom_text_repel(aes(x = V1, y = V2, label = rownames(df))) +
xlab("Dimension 1") +
ylab("Dimension 2 ") +
# geom_text(fontface = 2, alpha = .8) +
theme_bw(base_size = 12) +
theme(legend.position = "none") +
ggtitle(paste0("2D reduction of Word Embedding Model ", ref_name," using t_SNE"))
ggsave(paste0(ref_name, ".jpeg"), path = path, width = 24,
height = 18, dpi = 100)
ggplot(df) +
geom_point(aes(x = V1, y = V2), color = "red") +
geom_text_repel(aes(x = V1, y = V2, label = rownames(df))) +
xlab("Dimension 1") +
ylab("Dimension 2 ") +
# geom_text(fontface = 2, alpha = .8) +
theme_bw(base_size = 12) +
theme(legend.position = "none") +
ggtitle(paste0("2D reduction of Word Embedding Model on stack over flow data of pakistan using t_SNE"))
ggsave(paste0(ref_name, ".jpeg"), path = path, width = 24,
height = 18, dpi = 100)
ggplot(df) +
geom_point(aes(x = V1, y = V2), color = "red") +
geom_text_repel(aes(x = V1, y = V2, label = rownames(df))) +
xlab("Dimension 1") +
ylab("Dimension 2 ") +
# geom_text(fontface = 2, alpha = .8) +
theme_bw(base_size = 12) +
theme(legend.position = "none") +
ggtitle(paste0("2D reduction of Word Embedding Model on stack over flow data of pakistan using t_SNE"))
ggsave(paste0("without_lower", ".jpeg"), path = path, width = 24,
height = 18, dpi = 100)
ggplot(df) +
geom_point(aes(x = V1, y = V2), color = "red") +
geom_text_repel(aes(x = V1, y = V2, label = rownames(df))) +
xlab("Dimension 1") +
ylab("Dimension 2 ") +
# geom_text(fontface = 2, alpha = .8) +
theme_bw(base_size = 12) +
theme(legend.position = "none") +
ggtitle(paste0("2D reduction of Word Embedding Model on stack over flow data of pakistan using t_SNE"))
prep_word2vec(origin='features.txt',destination = 'vectors.txt',lowercase = T)
model = train_word2vec("vectors.txt","feature_vectors.bin",vectors=300,threads=4,window=5,iter=5,negative_samples=0)
word2vec = read.vectors("feature_vectors.bin")
reduction <- Rtsne(as.matrix(word2vec), dims = 2, initial_dims = 50,
perplexity = 21, theta = 0.5, check_duplicates = F,
pca = F, max_iter = 1000, verbose = F,
is_distance = F, Y_init = NULL)
df <- as.data.frame(reduction$Y)
rows <- rownames(word2vec)
rownames(df) <- rows
df=df %>% filter(rownames(df) != '</s>')
rownames(df) <- rows[2:67]
# Create t-SNE plot and save as jpeg
ggplot(df) +
geom_point(aes(x = V1, y = V2), color = "red") +
geom_text_repel(aes(x = V1, y = V2, label = rownames(df))) +
xlab("Dimension 1") +
ylab("Dimension 2 ") +
# geom_text(fontface = 2, alpha = .8) +
theme_bw(base_size = 12) +
theme(legend.position = "none") +
ggtitle(paste0("2D reduction of Word Embedding Model on stack over flow data of pakistan using t_SNE"))
model
model = train_word2vec("vectors.txt","feature_vectors.bin",vectors=300,threads=4,window=5,iter=5,negative_samples=0)
word2vec = read.vectors("feature_vectors.bin")
word2vec
reduction <- Rtsne(word2vec, dims = 2, initial_dims = 50,
perplexity = 21, theta = 0.5, check_duplicates = F,
pca = F, max_iter = 1000, verbose = F,
is_distance = F, Y_init = NULL)
df <- as.data.frame(reduction$Y)
rows <- rownames(word2vec)
rownames(df) <- rows
df=df %>% filter(rownames(df) != '</s>')
rownames(df) <- rows[2:67]
rows[2:65]
rows[2:66]
df=df %>% filter(rownames(df) != '</s>')
rownames(df) <- rows[2:65]
ggplot(df) +
geom_point(aes(x = V1, y = V2), color = "red") +
geom_text_repel(aes(x = V1, y = V2, label = rownames(df))) +
xlab("Dimension 1") +
ylab("Dimension 2 ") +
# geom_text(fontface = 2, alpha = .8) +
theme_bw(base_size = 12) +
theme(legend.position = "none") +
ggtitle(paste0("2D reduction of Word Embedding Model on stack over flow data of pakistan using t_SNE"))
reduction <- Rtsne(word2vec, dims = 2, initial_dims = 50,
perplexity = 21, theta = 0.5, check_duplicates = F,
pca = F, max_iter = 1000, verbose = F,
is_distance = F, Y_init = NULL)
df <- as.data.frame(reduction$Y)
rows <- rownames(word2vec)
ggplot(df) +
geom_point(aes(x = V1, y = V2), color = "red") +
geom_text_repel(aes(x = V1, y = V2, label = rownames(df))) +
xlab("Dimension 1") +
ylab("Dimension 2 ") +
# geom_text(fontface = 2, alpha = .8) +
theme_bw(base_size = 12) +
theme(legend.position = "none") +
ggtitle(paste0("2D reduction of Word Embedding Model on stack over flow data of pakistan using t_SNE"))
rows <- rownames(word2vec)
rownames(df) <- rows
ggplot(df) +
geom_point(aes(x = V1, y = V2), color = "red") +
geom_text_repel(aes(x = V1, y = V2, label = rownames(df))) +
xlab("Dimension 1") +
ylab("Dimension 2 ") +
# geom_text(fontface = 2, alpha = .8) +
theme_bw(base_size = 12) +
theme(legend.position = "none") +
ggtitle(paste0("2D reduction of Word Embedding Model on stack over flow data of pakistan using t_SNE"))
word2vec %>% closest_to('vim')
model
model = train_word2vec("vectors.txt","feature_vectors.bin",vectors=300,threads=4,window=5,iter=5,negative_samples=0)
model = train_word2vec("vectors.txt","feature_vectors.bin",vectors=300,threads=4,window=5,iter=5,negative_samples=0)
model
rownames(model)
rownames(model)[2:65]
model[2:65]
model[:,2:65]
model[,2:65]
model[2:65,]
model
model[2:65,]
word2vec=model[2:65,]  # remove at end of string
word2vec %>% closest_to('vim')
word2vec %>% closest_to('aws')
reduction <- Rtsne(word2vec, dims = 2, initial_dims = 50,
perplexity = 21, theta = 0.5, check_duplicates = F,
pca = F, max_iter = 1000, verbose = F,
is_distance = F, Y_init = NULL)
df <- as.data.frame(reduction$Y)
rows <- rownames(word2vec)
rownames(df) <- rows
#df=df %>% filter(rownames(df) != '</s>')
#rownames(df) <- rows[2:65]
# Create t-SNE plot and save as jpeg
ggplot(df) +
geom_point(aes(x = V1, y = V2), color = "red") +
geom_text_repel(aes(x = V1, y = V2, label = rownames(df))) +
xlab("Dimension 1") +
ylab("Dimension 2 ") +
# geom_text(fontface = 2, alpha = .8) +
theme_bw(base_size = 12) +
theme(legend.position = "none") +
ggtitle(paste0("2D reduction of Word Embedding Model on stack over flow data of pakistan using t_SNE"))
save(ggplot(df) +
geom_point(aes(x = V1, y = V2), color = "red") +
geom_text_repel(aes(x = V1, y = V2, label = rownames(df))) +
xlab("Dimension 1") +
ylab("Dimension 2 ") +
# geom_text(fontface = 2, alpha = .8) +
theme_bw(base_size = 12) +
theme(legend.position = "none") +
ggtitle(paste0("2D reduction of Word Embedding Model on stack over flow data of pakistan using t_SNE")),'a.png')
ggsave('s_removed.png')
word2vec = read.vectors("feature_vectors.bin")
reduction <- Rtsne(word2vec, dims = 2, initial_dims = 50,
perplexity = 21, theta = 0.5, check_duplicates = F,
pca = F, max_iter = 1000, verbose = F,
is_distance = F, Y_init = NULL)
df <- as.data.frame(reduction$Y)
rows <- rownames(word2vec)
rownames(df) <- rows
ggplot(df) +
geom_point(aes(x = V1, y = V2), color = "red") +
geom_text_repel(aes(x = V1, y = V2, label = rownames(df))) +
xlab("Dimension 1") +
ylab("Dimension 2 ") +
# geom_text(fontface = 2, alpha = .8) +
theme_bw(base_size = 12) +
theme(legend.position = "none") +
ggtitle(paste0("2D reduction of Word Embedding Model on stack over flow data of pakistan using t_SNE"))
ggsave('s_removed.png')
word2vec=model[2:65,]  # remove at end of string
# Create t-SNE plot and save as jpeg
ggplot(df) +
geom_point(aes(x = V1, y = V2), color = "red") +
geom_text_repel(aes(x = V1, y = V2, label = rownames(df))) +
xlab("Dimension 1") +
ylab("Dimension 2 ") +
# geom_text(fontface = 2, alpha = .8) +
theme_bw(base_size = 12) +
theme(legend.position = "none") +
ggtitle(paste0("2D reduction of Word Embedding Model on stack over flow data of pakistan using t_SNE"))
ggsave('s_removed.png')
ggplot(df) +
geom_point(aes(x = V1, y = V2), color = "red") +
geom_text_repel(aes(x = V1, y = V2, label = rownames(df))) +
xlab("Dimension 1") +
ylab("Dimension 2 ") +
# geom_text(fontface = 2, alpha = .8) +
theme_bw(base_size = 12) +
theme(legend.position = "none") +
ggtitle(paste0("2D reduction of Word Embedding Model on stack over flow data of pakistan using t_SNE"))
prep_word2vec(origin='features.txt',destination = 'vectors.txt',lowercase = T)
model = train_word2vec("vectors.txt","feature_vectors.bin",vectors=300,threads=4,window=5,iter=5,negative_samples=0)
word2vec=model[2:65,]  # remove at end of string
prep_word2vec(origin='features.txt',destination = 'vectors.txt',lowercase = T)
model = train_word2vec("vectors.txt","feature_vectors.bin",vectors=300,threads=4,window=5,iter=5,negative_samples=0)
word2vec=model[2:65,]  # remove at end of string
word2vec
reduction <- Rtsne(word2vec, dims = 2, initial_dims = 50,
perplexity = 21, theta = 0.5, check_duplicates = F,
pca = F, max_iter = 1000, verbose = F,
is_distance = F, Y_init = NULL)
df <- as.data.frame(reduction$Y)
rows <- rownames(word2vec)
rownames(df) <- rows
#df=df %>% filter(rownames(df) != '</s>')
#rownames(df) <- rows[2:65]
# Create t-SNE plot and save as jpeg
ggplot(df) +
geom_point(aes(x = V1, y = V2), color = "red") +
geom_text_repel(aes(x = V1, y = V2, label = rownames(df))) +
xlab("Dimension 1") +
ylab("Dimension 2 ") +
# geom_text(fontface = 2, alpha = .8) +
theme_bw(base_size = 12) +
theme(legend.position = "none") +
ggtitle(paste0("2D reduction of Word Embedding Model on stack over flow data of pakistan using t_SNE"))
ggsave('s_removed.png')
word2vec = read.vectors("feature_vectors.bin")
word2vec
reduction <- Rtsne(word2vec, dims = 2, initial_dims = 50,
perplexity = 21, theta = 0.5, check_duplicates = F,
pca = F, max_iter = 1000, verbose = F,
is_distance = F, Y_init = NULL)
df <- as.data.frame(reduction$Y)
rows <- rownames(word2vec)
rownames(df) <- rows
#df=df %>% filter(rownames(df) != '</s>')
#rownames(df) <- rows[2:65]
# Create t-SNE plot and save as jpeg
ggplot(df) +
geom_point(aes(x = V1, y = V2), color = "red") +
geom_text_repel(aes(x = V1, y = V2, label = rownames(df))) +
xlab("Dimension 1") +
ylab("Dimension 2 ") +
# geom_text(fontface = 2, alpha = .8) +
theme_bw(base_size = 12) +
theme(legend.position = "none") +
ggtitle(paste0("2D reduction of Word Embedding Model on stack over flow data of pakistan using t_SNE"))
ggsave('s_not_removed.png')
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
prep_word2vec(origin='features.txt',destination = 'vectors.txt',lowercase = T)
model = train_word2vec("vectors.txt",vectors=300,threads=4,window=5,iter=5,negative_samples=0)
word2vec=model[2:65,]  # remove at end of string
?Rtsne
reduction <- Rtsne(word2vec, dims = 2, initial_dims = 50,
perplexity = 21, theta = 0, check_duplicates = F,
pca = F, max_iter = 1000, verbose = T,
is_distance = F, Y_init = NULL)
df <- as.data.frame(reduction$Y)
rows <- rownames(word2vec)
rownames(df) <- rows
ggplot(df) +
geom_point(aes(x = V1, y = V2), color = "red") +
geom_text_repel(aes(x = V1, y = V2, label = rownames(df))) +
xlab("Dimension 1") +
ylab("Dimension 2 ") +
# geom_text(fontface = 2, alpha = .8) +
theme_bw(base_size = 12) +
theme(legend.position = "none") +
ggtitle(paste0("2D reduction of Word Embedding Model on stack over flow data of pakistan using t_SNE"))
reduction <- Rtsne(word2vec, dims = 2, initial_dims = 50,
perplexity = 21, theta = 0.5, check_duplicates = F,
pca = F, max_iter = 1000, verbose = T,
is_distance = F, Y_init = NULL)
df <- as.data.frame(reduction$Y)
rows <- rownames(word2vec)
rownames(df) <- rows
ggplot(df) +
geom_point(aes(x = V1, y = V2), color = "red") +
geom_text_repel(aes(x = V1, y = V2, label = rownames(df))) +
xlab("Dimension 1") +
ylab("Dimension 2 ") +
# geom_text(fontface = 2, alpha = .8) +
theme_bw(base_size = 12) +
theme(legend.position = "none") +
ggtitle(paste0("2D reduction of Word Embedding Model on stack over flow data of pakistan using t_SNE"))
ggsave('wordembeddings.png')
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
write( embeddingAllHave,'have_features.txt')
prep_word2vec(origin='have_features.txt',destination = 'vectors.txt',lowercase = T)
model = train_word2vec("vectors.txt",vectors=300,threads=4,window=5,iter=5,negative_samples=0)
word2vec=model[2:65,]  # remove at end of string
#tsne (dimensionality reduction)
reduction <- Rtsne(word2vec, dims = 2, initial_dims = 50,
perplexity = 21, theta = 0.5, check_duplicates = F,
pca = F, max_iter = 1000, verbose = T,
is_distance = F, Y_init = NULL)
df <- as.data.frame(reduction$Y)
rows <- rownames(word2vec)
rownames(df) <- rows
#df=df %>% filter(rownames(df) != '</s>')
#rownames(df) <- rows[2:65]
# Create t-SNE plot and save as jpeg
ggplot(df) +
geom_point(aes(x = V1, y = V2), color = "red") +
geom_text_repel(aes(x = V1, y = V2, label = rownames(df))) +
xlab("Dimension 1") +
ylab("Dimension 2 ") +
# geom_text(fontface = 2, alpha = .8) +
theme_bw(base_size = 12) +
theme(legend.position = "none") +
ggtitle(paste0("2D reduction of Word Embedding Model on stack over flow data of pakistan using t_SNE"))
ggsave('wordembeddings_have.png')
